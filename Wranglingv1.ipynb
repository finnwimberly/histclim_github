{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f9f1bf-97c0-4775-b96c-c186a08af5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wrangling.ipynb', '.ipynb_checkpoints', 'data_old_files.txt', 'data_files.txt', 'folder_structure', 'folder_structure.pdf']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from graphviz import Digraph\n",
    "import os\n",
    "\n",
    "# List the contents of the current directory\n",
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c673a-d259-43ef-bf70-4b5d8b6a22cf",
   "metadata": {},
   "source": [
    "First we will visualize the old data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d88c4301-b96f-4052-8e57-da821e24633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file paths from the text file\n",
    "file_paths_txt = \"data_old_files.txt\"  # Adjust as needed\n",
    "\n",
    "def read_file_paths(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        file_paths = [line.strip() for line in f]\n",
    "    return file_paths\n",
    "\n",
    "file_paths = read_file_paths(file_paths_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8cbc5da-bd5c-4d90-9256-6c605adf49b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'folder_structure.pdf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to count files in each folder\n",
    "def create_folder_structure(file_paths):\n",
    "    folder_dict = defaultdict(list)\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        parts = file_path.strip().split('/')\n",
    "        folder_path = '/'.join(parts[:-1])  # Folder path without file name\n",
    "        folder_dict[folder_path].append(parts[-1])  # Append file name to folder\n",
    "    \n",
    "    return folder_dict\n",
    "\n",
    "# Function to generate the folder structure graph\n",
    "def create_graph(folder_dict):\n",
    "    dot = Digraph(comment='Folder Structure')\n",
    "    dot.attr(rankdir='TB', ranksep='2', nodesep='0.5')  # Set to vertical orientation and adjust spacing\n",
    "    \n",
    "    for folder, files in folder_dict.items():\n",
    "        subfolders = folder.split('/')\n",
    "        \n",
    "        # Create nodes for the folder hierarchy\n",
    "        for i in range(1, len(subfolders)):\n",
    "            parent = '/'.join(subfolders[:i])\n",
    "            child = '/'.join(subfolders[:i+1])\n",
    "            dot.node(child, label=subfolders[i])\n",
    "            dot.edge(parent, child)\n",
    "        \n",
    "        # Add the file count at the lowest-level folder\n",
    "        file_count = len(files)\n",
    "        dot.node(folder, label=f\"{subfolders[-1]} ({file_count} files)\")\n",
    "    \n",
    "    return dot\n",
    "\n",
    "# Assuming `file_paths` contains the list of absolute file paths\n",
    "file_paths = []\n",
    "with open(\"data_files.txt\", 'r') as f:\n",
    "    file_paths = [line.strip() for line in f]\n",
    "\n",
    "folder_dict = create_folder_structure(file_paths)\n",
    "graph = create_graph(folder_dict)\n",
    "graph.attr(dpi='300')  # Set DPI to 300 for high resolution\n",
    "graph.render('folder_structure', format='pdf')  # Save as PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97daf283-137a-452a-a812-7459360fc7ca",
   "metadata": {},
   "source": [
    "This worked okay... but the data set is quite large and the structure is complicated. It will likely be more comprehensive to just filter through what files have already been transferred over to the new data structure and which have not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad7ccf3-8fa6-4b3e-ad00-3eba2b70fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the text files (inside our project folder)\n",
    "data_old_path = \"data_old_files.txt\"\n",
    "data_path = \"data_files.txt\"\n",
    "\n",
    "# Read file names from text files\n",
    "def read_file_names(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Strip whitespace and get the last part of the path (file name)\n",
    "        file_names = ['/'.join(line.strip().split('/')[-1:]) for line in f]\n",
    "\n",
    "    return file_names\n",
    "\n",
    "# Read file names from text files\n",
    "data_old_files1 = read_file_names(data_old_path)\n",
    "data_files1 = read_file_names(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9b90b1c-d371-4ffb-86e6-d9ea43732b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The above function works like this\n",
    "path_to_file = 'this/is/my/path/until/my/file'\n",
    "'/'.join(path_to_file.strip().split('/')[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3393ca4-db01-490a-b92c-43417c61d349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate file names in data_old_files: 84\n",
      "Total number of duplicate occurrences in data_old_files: 261\n",
      "ensemble1.nc: 3 occurrences\n",
      "ensemble10.nc: 3 occurrences\n",
      "ensemble11.nc: 3 occurrences\n",
      "ensemble12.nc: 3 occurrences\n",
      "ensemble13.nc: 3 occurrences\n",
      "ensemble2.nc: 3 occurrences\n",
      "ensemble3.nc: 3 occurrences\n",
      "ensemble4.nc: 3 occurrences\n",
      "ensemble5.nc: 3 occurrences\n",
      "ensemble6.nc: 3 occurrences\n",
      "ensemble7.nc: 3 occurrences\n",
      "ensemble8.nc: 3 occurrences\n",
      "ensemble9.nc: 3 occurrences\n",
      "out_0_ensemble1.nc: 5 occurrences\n",
      "out_0_ensemble10.nc: 5 occurrences\n",
      "out_0_ensemble11.nc: 5 occurrences\n",
      "out_0_ensemble12.nc: 5 occurrences\n",
      "out_0_ensemble13.nc: 5 occurrences\n",
      "out_0_ensemble2.nc: 5 occurrences\n",
      "out_0_ensemble3.nc: 5 occurrences\n",
      "out_0_ensemble4.nc: 5 occurrences\n",
      "out_0_ensemble5.nc: 5 occurrences\n",
      "out_0_ensemble6.nc: 5 occurrences\n",
      "out_0_ensemble7.nc: 5 occurrences\n",
      "out_0_ensemble8.nc: 5 occurrences\n",
      "out_0_ensemble9.nc: 5 occurrences\n",
      "out_1_ensemble1.nc: 5 occurrences\n",
      "out_1_ensemble10.nc: 5 occurrences\n",
      "out_1_ensemble11.nc: 5 occurrences\n",
      "out_1_ensemble12.nc: 5 occurrences\n",
      "out_1_ensemble13.nc: 5 occurrences\n",
      "out_1_ensemble2.nc: 5 occurrences\n",
      "out_1_ensemble3.nc: 5 occurrences\n",
      "out_1_ensemble4.nc: 5 occurrences\n",
      "out_1_ensemble5.nc: 5 occurrences\n",
      "out_1_ensemble6.nc: 5 occurrences\n",
      "out_1_ensemble7.nc: 5 occurrences\n",
      "out_1_ensemble8.nc: 5 occurrences\n",
      "out_1_ensemble9.nc: 5 occurrences\n",
      "out_2_ensemble1.nc: 5 occurrences\n",
      "out_2_ensemble10.nc: 5 occurrences\n",
      "out_2_ensemble11.nc: 5 occurrences\n",
      "out_2_ensemble12.nc: 5 occurrences\n",
      "out_2_ensemble13.nc: 5 occurrences\n",
      "out_2_ensemble2.nc: 5 occurrences\n",
      "out_2_ensemble3.nc: 5 occurrences\n",
      "out_2_ensemble4.nc: 5 occurrences\n",
      "out_2_ensemble5.nc: 5 occurrences\n",
      "out_2_ensemble6.nc: 5 occurrences\n",
      "out_2_ensemble7.nc: 5 occurrences\n",
      "out_2_ensemble8.nc: 5 occurrences\n",
      "out_2_ensemble9.nc: 5 occurrences\n",
      "out_3_ensemble1.nc: 5 occurrences\n",
      "out_3_ensemble10.nc: 5 occurrences\n",
      "out_3_ensemble11.nc: 5 occurrences\n",
      "out_3_ensemble12.nc: 5 occurrences\n",
      "out_3_ensemble13.nc: 5 occurrences\n",
      "out_3_ensemble2.nc: 5 occurrences\n",
      "out_3_ensemble3.nc: 5 occurrences\n",
      "out_3_ensemble4.nc: 5 occurrences\n",
      "out_3_ensemble5.nc: 5 occurrences\n",
      "out_3_ensemble6.nc: 5 occurrences\n",
      "out_3_ensemble7.nc: 5 occurrences\n",
      "out_3_ensemble8.nc: 5 occurrences\n",
      "out_3_ensemble9.nc: 5 occurrences\n",
      "wget-log: 2 occurrences\n",
      "SST112923.png: 2 occurrences\n",
      "README.md: 3 occurrences\n",
      "JD104_piCtrl_RegionCut_2S10S_107E120E_top700m.tgz: 2 occurrences\n",
      "JD105_histssp585_RegionCut_2S10S_107E120E_top700m.tgz: 2 occurrences\n",
      "._README.md: 2 occurrences\n",
      "mask.nc: 2 occurrences\n",
      "mesh_hgr.nc: 2 occurrences\n",
      "mesh_zgr.nc: 2 occurrences\n",
      "meshmask_50w.nc: 3 occurrences\n",
      "new_maskglo.nc: 2 occurrences\n",
      "u50w.nc: 3 occurrences\n",
      "ORCA025.L46.LIM2vp.CFCSF6.JRA.XIOS2-K004.thermhal90_1m_19580101_20161231_NWAtl_grid_V.nc: 2 occurrences\n",
      "ORCA025.L46.LIM2vp.CFCSF6.JRA.XIOS2-K003.hindcast_mesh_mask.README: 2 occurrences\n",
      "mask_EIO.nc: 2 occurrences\n",
      "mesh_hgr_EIO.nc: 2 occurrences\n",
      "mesh_zgr_EIO.nc: 2 occurrences\n",
      "readme.txt: 6 occurrences\n",
      "map_time_robust_corr_aug2015v50_V4.mat: 3 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Function to filter out .DS_Store files\n",
    "def filter_out_ds_store(file_list):\n",
    "    return [file for file in file_list if not (file.endswith('.DS_Store') or file.endswith('._.DS_Store'))]\n",
    "\n",
    "# Filter out .DS_Store files from both lists\n",
    "data_old_files = filter_out_ds_store(data_old_files1)\n",
    "data_files = filter_out_ds_store(data_files1)\n",
    "\n",
    "# Count occurrences of each file name\n",
    "file_name_counts_old = Counter(data_old_files)\n",
    "\n",
    "# Find file names with duplicates in data_old_files\n",
    "duplicate_file_names_old = {file_name: count for file_name, count in file_name_counts_old.items() if count > 1}\n",
    "\n",
    "# Number of duplicate file names in data_old_files\n",
    "num_duplicate_file_names_old = len(duplicate_file_names_old)\n",
    "total_duplicates_old = sum(count - 1 for count in duplicate_file_names_old.values())\n",
    "\n",
    "print(f\"Number of duplicate file names in data_old_files: {num_duplicate_file_names_old}\")\n",
    "print(f\"Total number of duplicate occurrences in data_old_files: {total_duplicates_old}\")\n",
    "\n",
    "# Optional: Display file names and their counts\n",
    "for file_name, count in duplicate_file_names_old.items():\n",
    "    print(f\"{file_name}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05ecee-dff2-4a60-b18e-d8d207cacdb5",
   "metadata": {},
   "source": [
    "For now we will remove the duplicates from data_old_files. We can then easily automate the transfer of files with non-duplicate names and deal with the more difficult duplicates later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d061067-0738-4b2b-b4d9-11f939da5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique file names by using a set to filter duplicates\n",
    "unique_data_old_files = list(set(data_old_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d89a7654-ea58-477f-b8ca-840ac04d38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see which (of our non-duplicate) files have already been transferred and which have not\n",
    "# Convert lists of lists to sets of tuples for easier comparison\n",
    "def convert_to_set_of_tuples(file_list):\n",
    "    return set(tuple(file) for file in file_list)\n",
    "\n",
    "# Convert lists to sets of tuples\n",
    "data_old_set = convert_to_set_of_tuples(unique_data_old_files)\n",
    "data_set = convert_to_set_of_tuples(data_files)\n",
    "\n",
    "# Files in data_old_files but not in data_files\n",
    "files_only_in_old = data_old_set - data_set\n",
    "\n",
    "# Files in both data_old_files and data_files\n",
    "files_in_both = data_old_set & data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4766bb83-79a4-42de-8a3b-86a3b0da9871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6874 files have been transferred over from data_old to data.\n",
      "13550 files exist only in data_old.\n",
      "data_old contains 20424 total files (with unique names).\n",
      "6874 + 13550 = 20424\n"
     ]
    }
   ],
   "source": [
    "print(str(len(files_in_both)) + ' files have been transferred over from data_old to data.') \n",
    "print(str(len(files_only_in_old)) + ' files exist only in data_old.')\n",
    "print('data_old contains ' + str(len(unique_data_old_files)) + ' total files (with unique names).')\n",
    "print('6874 + 13550 = ' + str(6874 + 13550))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44be462-356d-4d0a-a128-6c95aa52b9ce",
   "metadata": {},
   "source": [
    "Now we want to consider how to move all of these old files into data. To make sure we aren't missing any files, lets check that out of the files with names appearing in both data_old and data, there are no duplicates in data (ie that we are not assuming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "523534f4-e1a6-44da-8106-2042b5fdd9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recurrent names in both data and data_old: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if any duplicate file names exist in both data and data_old\n",
    "common_file_names = set(data_files).intersection(files_in_both)\n",
    "\n",
    "# Number of common file names\n",
    "num_common_file_names = len(common_file_names)\n",
    "\n",
    "print(f\"Number of recurrent names in both data and data_old: {num_common_file_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90a4e07b-291e-405d-8e0f-efd548e953ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20424"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "This means that none of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45adb0-76c1-4367-b29f-6a9040bbb7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(fiwi)",
   "language": "python",
   "name": "fiwi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
