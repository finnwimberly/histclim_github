{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c703bd05-dd41-4d40-b1d3-2a838bd295ce",
   "metadata": {},
   "source": [
    "This notebook sorts through which files have already been transferred into the new data (clidex/data/) structure and which remain exclusively in the old structure (clidex/data_old/). Files which have not yet been transferred to the new repository are  sorted into groups corresponding to folders of the new structure before being exported to clidex/data_old/. \n",
    "\n",
    "Last update: 23 Sep 2024 | FFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e7d4ed-fed4-461d-9365-60e0e35ca06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from collections import defaultdict, Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import shutil \n",
    "import pandas as pd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f6cf9ef9-56f2-4d28-a401-56d09b4b728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to generate a hash for the first MB of the file\n",
    "# def hash_partial_file(file_path, chunk_size=1024*1024):  # Default chunk size is 1 MB\n",
    "#     \"\"\"Generate a hash for only a portion of the file.\"\"\"\n",
    "#     hasher = hashlib.sha256()  # Or use hashlib.md5() if you prefer a faster, less secure hash\n",
    "#     with open(file_path, 'rb') as f:\n",
    "#         chunk = f.read(chunk_size)  # Read only the first chunk_size bytes\n",
    "#         hasher.update(chunk)\n",
    "#     return hasher.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1c806814-485f-4a65-b2e6-12c4882d8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to hash file and store result, or log if permission denied\n",
    "# def hash_and_store_partial(file_name, directory, chunk_size=1024*1024, denied_list=[]):\n",
    "#     \"\"\"Hash a file and store the result if it exists, or log permission denied files.\"\"\"\n",
    "#     file_path = os.path.join(directory, file_name)\n",
    "#     try:\n",
    "#         if os.path.isfile(file_path):\n",
    "#             return (file_name, hash_partial_file(file_path, chunk_size))\n",
    "#     except PermissionError:\n",
    "#         print(f\"Permission denied: {file_path}\")\n",
    "#         denied_list.append(file_name)  # Add to permission denied list\n",
    "#         return None  # Skip this file\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "905974d3-ad06-4482-93dc-f854374a06f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read file names from text files\n",
    "# def read_file_names(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         # Read each line, strip whitespace, and keep the full path\n",
    "#         file_names = [line.strip() for line in f]\n",
    "#     return file_names\n",
    "\n",
    "# # Paths to the text files (inside your project folder)\n",
    "# data_old_path = 'File Lists/data_old_files.txt'\n",
    "# data_path = 'File Lists/data_files.txt'\n",
    "\n",
    "# # Read file names from text files\n",
    "# data_old_files1 = read_file_names(data_old_path)\n",
    "# data_files1 = read_file_names(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "365b364d-4128-427c-a3b8-38338af05cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to filter out .DS_Store files\n",
    "# def filter_out_ds_store(file_list):\n",
    "#     return [file for file in file_list if not (file.endswith('.DS_Store') or file.endswith('._.DS_Store'))]\n",
    "\n",
    "# # Filter out .DS_Store files from both lists\n",
    "# data_old_files = filter_out_ds_store(data_old_files1)\n",
    "# data_files = filter_out_ds_store(data_files1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ef9be4c8-9f57-49ab-9443-d813aa66a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the paths to your data directories\n",
    "# data_old_directory = '/vast/clidex/'\n",
    "\n",
    "# # Get file hashes for all filtered files in data_old using multiple threads and partial hashing\n",
    "# data_old_hashes = []\n",
    "# chunk_size = 1024 * 1024  # 1 MB chunk\n",
    "\n",
    "# with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "#     # Map each file to the hash_and_store_partial function\n",
    "#     results = executor.map(lambda file_name: hash_and_store_partial(file_name, data_old_directory, chunk_size), data_old_files)\n",
    "\n",
    "#     # Collect non-None results\n",
    "#     data_old_hashes = [res for res in results if res is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208caf7e-5515-40e4-a0fd-90ce22e91a0d",
   "metadata": {},
   "source": [
    "Now we can move on to see what files have been moved over successfullly and which have not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0886c834-6ad1-423f-a2ba-c2082ab25a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the base directory for data\n",
    "# base_directory = '/vast/clidex/data'\n",
    "\n",
    "# # Get file hashes for all files in 'data' using absolute paths\n",
    "# data_hashes = []\n",
    "# denied_files = []  # Keep track of permission denied files in 'data'\n",
    "\n",
    "# # Create absolute paths for files in 'data'\n",
    "# data_files_absolute = [os.path.join(base_directory, file.lstrip('./')) for file in data_files]\n",
    "\n",
    "# with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "#     results_data = executor.map(lambda file_path: hash_and_store_partial(file_path, base_directory, chunk_size, denied_files), data_files_absolute)\n",
    "#     data_hashes = [res for res in results_data if res is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e13f1a9c-6d17-48af-8c09-14fe040057a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comparison: Find files in data_old that are missing from data\n",
    "# data_old_hashes_set = set(file_hash for _, file_hash in data_old_hashes)\n",
    "# data_hashes_set = set(file_hash for _, file_hash in data_hashes)\n",
    "\n",
    "# # Files that are in 'data_old' but not in 'data'\n",
    "# only_in_old = [file_name for file_name, file_hash in data_old_hashes if file_hash not in data_hashes_set]\n",
    "# in_both = [file_name for file_name, file_hash in data_old_hashes if file_hash in data_hashes_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26f25cd0-43df-47a0-8231-6a535eab6e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6962 files have been transferred over from data_old to data.\n",
      "13721 files exist only in data_old.\n",
      "data_old contains 20685 total files.\n",
      "6962 + 13721 = 20683\n"
     ]
    }
   ],
   "source": [
    "print(str(len(in_both)) + ' files have been transferred over from data_old to data.') \n",
    "print(str(len(only_in_old)) + ' files exist only in data_old.')\n",
    "print('data_old contains ' + str(len(data_old_files)) + ' total files.')\n",
    "print('6962 + 13721 = ' + str(6962 + 13721))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a988c63c-42b0-4f72-a861-b57a91716e01",
   "metadata": {},
   "source": [
    "Here the discrepancy of two relates to the two files which I did not have acess to when hashing all files in data_old. These are /vast/clidex/data_old/CESM_HR/Zusatzmaterial/CESM1_HR_PICONTROL.cvdp_data.150-501.nc-20230824T194710Z-001.zip and /vast/clidex/data_old/CESM_HR/Zusatzmaterial/POP_tx0.1v3_grid-001.nc - will inquire about them later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b81f59-d7b5-43cf-984b-cea4b9a5efbf",
   "metadata": {},
   "source": [
    "Before proceeding lets save the file lists so that we do not have to perform the (somewhat long hashing again):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ae2d722f-c963-4dc9-8d0a-b9c36165c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define paths for the output files\n",
    "# only_in_old_path = 'File Lists/only_in_old.txt'\n",
    "# in_both_path = 'File Lists/in_both.txt'\n",
    "\n",
    "# # Save lists to text files\n",
    "# with open(only_in_old_path, 'w') as f:\n",
    "#     for file_name in only_in_old:\n",
    "#         f.write(f\"{file_name}\\n\")\n",
    "\n",
    "# with open(in_both_path, 'w') as f:\n",
    "#     for file_name in in_both:\n",
    "#         f.write(f\"{file_name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659b07b-7958-41e0-a33b-9eb1dd71e90f",
   "metadata": {},
   "source": [
    "Now lets start organizing file names into groups based upon what data they give and WHERE we want them to end up in the new data structure. Reading in the saved files (so that you only have to run this half of the notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ebd9ab-b880-497b-b9d4-1a6010d989b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for the input files\n",
    "only_in_old_path = 'File Lists/only_in_old.txt'\n",
    "in_both_path = 'File Lists/in_both.txt'\n",
    "\n",
    "# Read the list of files only in old\n",
    "with open(only_in_old_path, 'r') as f:\n",
    "    only_in_old = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Read the list of files in both\n",
    "with open(in_both_path, 'r') as f:\n",
    "    in_both = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57957e3a-6c18-48dd-91a3-f4269aab13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OCEAN_3D_keys = ['ARGO', 'CARS', 'XBT', 'WOA', 'QuOTA', 'OHC', 'INSTANT_ARRAY']\n",
    "NEMO_keys = ['forcing', 'VIKING', 'FOCI', 'ORCA', 'TROPAC', 'NUSA20', 'INDRANI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d78c99f-d429-437d-850a-d92b6091fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_sorting_criteria(file_list):\n",
    "    sorting_suggestions = defaultdict(list)\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        # Split the path and get all folder names\n",
    "        parts = file_path.split('/')\n",
    "        \n",
    "        # Extract relevant folder names (e.g., ignoring 'data_old' or 'CESM_CAM5_LME')\n",
    "        relevant_parts = [part for part in parts if part not in ['data_old', 'CESM_CAM5_LME']]\n",
    "\n",
    "        # Use the first folder name as the primary sort key\n",
    "        if relevant_parts:\n",
    "            primary_key = relevant_parts[0]\n",
    "            sorting_suggestions[primary_key].append(file_path)\n",
    "\n",
    "            # Check for additional potential sorting criteria\n",
    "            if len(relevant_parts) > 1:\n",
    "                secondary_key = relevant_parts[1]\n",
    "                sorting_suggestions[primary_key].append(f\"Also consider sorting by '{secondary_key}' for: {file_path}\")\n",
    "\n",
    "    return dict(sorting_suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9ae1b90-d4d0-4c10-9a2a-25b3e2093377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your keywords\n",
    "OCEAN_3D_keys = ['ARGO', 'CARS', 'XBT', 'WOA', 'QuOTA', 'OHC', 'INSTANT_ARRAY']\n",
    "NEMO_keys = ['forcing', 'VIKING', 'FOCI', 'ORCA', 'TROPAC', 'NUSA20', 'INDRANI']\n",
    "\n",
    "# Initialize a dictionary to store sorted files\n",
    "sorted_files = {key: [] for key in OCEAN_3D_keys + NEMO_keys}\n",
    "\n",
    "# Example initialization of file_list and all_keywords\n",
    "file_list = only_in_old\n",
    "all_keywords = OCEAN_3D_keys + NEMO_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8e10ddf-6c4c-4c41-9402-3188137dc9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_files_by_keywords(file_list, keywords):\n",
    "    \"\"\"\n",
    "    Sort files into groups based on the earliest matching keyword in the file path.\n",
    "\n",
    "    Parameters:\n",
    "    - file_list: List of file names to be sorted.\n",
    "    - keywords: List of keywords to use for sorting.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with keywords as keys and sets of matching file names as values.\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to store sorted files as sets\n",
    "    sorted_files = {key: set() for key in keywords}\n",
    "\n",
    "    # Sort files into groups based on keywords\n",
    "    for file_name in file_list:\n",
    "        best_match_key = None\n",
    "        best_match_index = float('inf')  # Start with a very large number\n",
    "        \n",
    "        for key in sorted_files.keys():\n",
    "            # Check if the key is in the file name (case insensitive)\n",
    "            if key.lower() in file_name.lower():\n",
    "                # Find the index of the first occurrence of the keyword in the file path\n",
    "                match_index = file_name.lower().index(key.lower())\n",
    "                # If this match is earlier than the best match found so far, update\n",
    "                if match_index < best_match_index:\n",
    "                    best_match_index = match_index\n",
    "                    best_match_key = key\n",
    "\n",
    "        # If a best match key was found, add the file to that category\n",
    "        if best_match_key:\n",
    "            sorted_files[best_match_key].add(file_name)\n",
    "\n",
    "    return sorted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ade1e4-f2f8-44cb-8a4c-3898008126ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of sorted files:\n",
      "Sorted by 'ARGO': 344 files\n",
      "Sorted by 'CARS': 2 files\n",
      "Sorted by 'XBT': 25 files\n",
      "Sorted by 'WOA': 1 files\n",
      "Sorted by 'QuOTA': 2 files\n",
      "Sorted by 'OHC': 1926 files\n",
      "Sorted by 'INSTANT_ARRAY': 12 files\n",
      "Sorted by 'forcing': 665 files\n",
      "Sorted by 'VIKING': 127 files\n",
      "Sorted by 'ORCA': 2687 files\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Which category would you like to see detailed lists for? (type 'exit' to quit)  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the search...\n"
     ]
    }
   ],
   "source": [
    "# Ensure file_list and all_keywords are defined\n",
    "if 'file_list' in locals() and 'all_keywords' in locals():\n",
    "    # Call the function\n",
    "    sorted_files = sort_files_by_keywords(file_list, all_keywords)\n",
    "\n",
    "    # Display a summary of the sorted files\n",
    "    summary = {key: len(files) for key, files in sorted_files.items() if files}\n",
    "\n",
    "    print(\"Summary of sorted files:\")\n",
    "    for key, count in summary.items():\n",
    "        print(f\"Sorted by '{key}': {count} files\")\n",
    "\n",
    "    # Prompt user to specify a category to display detailed lists\n",
    "    while True:\n",
    "        category_to_display = input(\"Which category would you like to see detailed lists for? (type 'exit' to quit) \").strip().replace(\"'\", \"\").lower()\n",
    "\n",
    "        # Allow the user to exit\n",
    "        if category_to_display in ['exit', 'quit']:\n",
    "            print(\"Exiting the search...\")\n",
    "            break  # Exit the loop and stop the operation\n",
    "\n",
    "        # Check against the lowercase keys of sorted_files\n",
    "        lowercase_sorted_keys = {key.lower(): key for key in sorted_files.keys()}\n",
    "\n",
    "        if category_to_display in lowercase_sorted_keys:\n",
    "            original_key = lowercase_sorted_keys[category_to_display]\n",
    "            files = sorted_files[original_key]\n",
    "            if files:\n",
    "                print(f\"\\nFiles sorted by '{original_key}':\")\n",
    "                for file_name in files:\n",
    "                    print(file_name)\n",
    "            else:\n",
    "                print(f\"No files found for category '{original_key}'.\")\n",
    "            break  # Exit the loop after displaying the files\n",
    "        else:\n",
    "            print(f\"Category '{category_to_display}' not recognized. Please try again.\")\n",
    "else:\n",
    "    print(\"Ensure that 'file_list' and 'all_keywords' are defined before calling the function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5596ea78-33ed-49f7-b0df-d022cb84a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store results\n",
    "sorted_files = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dab9a22d-262e-44dc-a3a0-4b88d51d69db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the search.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Clear the previous output (except the current input prompt)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Prompt user for input string\n",
    "    search_string = input(\"Enter a string to search for in file names (or type 'exit' to quit): \").strip().lower().replace(\"'\", \"\").replace('\"', '')\n",
    "\n",
    "    # Allow the user to exit the loop\n",
    "    if search_string == 'exit':\n",
    "        clear_output(wait=True)  # Clear the output before exiting\n",
    "        print(\"Exiting the search.\")\n",
    "        break\n",
    "\n",
    "    # Find matching files\n",
    "    matching_files = [file_name for file_name in file_list if search_string in file_name.lower()]\n",
    "\n",
    "    # Output the results\n",
    "    if matching_files:\n",
    "        print(f\"There are {len(matching_files)} files containing this keyword.\")\n",
    "        \n",
    "        # Ask if the user wants to see the full list\n",
    "        show_full_list = input(\"Would you like to see the full list of matching files? (yes/no): \").strip().lower()\n",
    "        \n",
    "        if show_full_list == 'yes':\n",
    "            print(f\"Files containing '{search_string}':\")\n",
    "            for file_name in matching_files:\n",
    "                print(file_name)\n",
    "\n",
    "        # Ask for the range of files to append\n",
    "        append_to_dict = input(\"Would you like to append these results to the dictionary? (yes/no): \").strip().lower()\n",
    "        \n",
    "        if append_to_dict == 'yes':\n",
    "            range_input = input(\"Enter the range of files to add (e.g., 4: for all from index 4 onwards or 2:5 for a specific range): \")\n",
    "\n",
    "            try:\n",
    "                # Parse the range input\n",
    "                if ':' in range_input:\n",
    "                    if range_input.count(':') == 1:  # Handling cases like \"4:\", \"2:5\"\n",
    "                        start_index, end_index = range_input.split(':')\n",
    "                        start_index = int(start_index) if start_index else 0  # Default to 0 if start is empty\n",
    "                        end_index = int(end_index) if end_index else None  # None means to the end\n",
    "                        selected_files = matching_files[start_index:end_index]\n",
    "                    else:\n",
    "                        raise ValueError\n",
    "                else:  # If no colon, add the whole list\n",
    "                    selected_files = matching_files\n",
    "\n",
    "                # Store selected files in the dictionary\n",
    "                sorted_files[search_string.upper()] = pd.DataFrame(selected_files, columns=[\"Matching Files\"])\n",
    "                print(f\"Results stored under key '{search_string.upper()}'.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid range input. Please use the format 'start:end'.\")\n",
    "    else:\n",
    "        print(f\"No files found containing '{search_string}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93c65d-ed29-405a-b6ff-8590452751cc",
   "metadata": {},
   "source": [
    "We can look at our dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f45c0db5-13df-402c-895c-bdc8f77bab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can check our dict\n",
    "#sorted_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee562c5-ffa5-40de-ae69-8eab7da62785",
   "metadata": {},
   "source": [
    "Making sure we did not add any files to multiple dict dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e1850ad-8b8c-434d-b303-0a46a108c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(sorted_files):\n",
    "    seen_files = set()\n",
    "    duplicates = set()\n",
    "\n",
    "    # Iterate through each key in the dictionary\n",
    "    for key in sorted_files:\n",
    "        for file_name in sorted_files[key]:\n",
    "            # Skip unwanted entries (like column names)\n",
    "            if file_name == \"Matching Files\":\n",
    "                continue\n",
    "            \n",
    "            # Normalize file name\n",
    "            file_name = str(file_name).strip()\n",
    "            \n",
    "            # Check if the file name has already been seen\n",
    "            if file_name in seen_files:\n",
    "                duplicates.add(file_name)\n",
    "            else:\n",
    "                seen_files.add(file_name)\n",
    "\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e96e75e6-42d5-43e9-b386-1d8c4c321e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate files found.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "duplicates = find_duplicates(sorted_files)\n",
    "\n",
    "if duplicates:\n",
    "    print(\"Duplicate files found:\")\n",
    "    for file in duplicates:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"No duplicate files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44a2dd8c-6894-48db-a3f8-6f3c8eee779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets save these lists of sorted files as txt files to use in our terminal\n",
    "\n",
    "# Iterate over the dictionary\n",
    "for key, df in sorted_files.items():\n",
    "    # Extract the list of file paths (assuming 'Matching Files' is the column name)\n",
    "    file_paths = sorted_files[key].values.tolist()\n",
    "    # Flatten the list (extract the string from each inner list)\n",
    "    file_paths = [path[0] if isinstance(path, list) else path for path in file_paths]\n",
    "    \n",
    "    # Define the text file name using the key\n",
    "    txt_filename = f\"sorted_txt_files/{key}_files.txt\"\n",
    "    \n",
    "    # # Write the file paths to the text file\n",
    "    # with open(txt_filename, 'w') as file:\n",
    "    #     for path in file_paths:\n",
    "    #         file.write(f\"{path}\\n\")\n",
    "\n",
    "     # Write the file paths to the text file with the absolute path\n",
    "    with open(txt_filename, 'w') as file:\n",
    "        for path in file_paths:\n",
    "            absolute_path = f\"/vast/clidex/{path}\"  # Prepend /vast/clidex/ to each path\n",
    "            file.write(f\"{absolute_path}\\n\")\n",
    "    \n",
    "    #print(f\"Saved {txt_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f476d335-338f-4f24-b0d8-f4585b90c760",
   "metadata": {},
   "source": [
    "I have worked through most of the keywords I know we want to move files for given our new data structure outline (https://docs.google.com/drawings/d/1hdjjVtAG1EScDvX2TAPQTSizGjYRxCvb2byInvkRNBI/edit). Now, let's see how many of the ~20,000 files have we accounted for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b6c6039-cfd7-4cc8-8bb0-8f52b49ca64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique files: 12877\n"
     ]
    }
   ],
   "source": [
    "unique_files = set()  # Use a set to automatically eliminate duplicates\n",
    "for key in sorted_files.keys():\n",
    "    unique_files.update(sorted_files[key]['Matching Files'])\n",
    "print(\"Total unique files:\", len(unique_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef457a0a-4996-459e-8172-c52b12effb2b",
   "metadata": {},
   "source": [
    "Let's see what is remaining and try and figure out where those belong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7db3e7ed-35a4-4730-8e5f-3b4e1e9ba323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the values in sorted_files and converting them to a set\n",
    "sorted_files_list = []\n",
    "for key in sorted_files.keys():\n",
    "    # Extracting the 'Matching Files' column from the DataFrame\n",
    "    sorted_files_list.extend(sorted_files[key]['Matching Files'].values)\n",
    "\n",
    "# Convert the flattened list to a set\n",
    "sorted_files_set = set(sorted_files_list)\n",
    "only_in_old_set = set(only_in_old)\n",
    "\n",
    "# Find the files that are in only_in_old but not in sorted_files\n",
    "unsorted_files = only_in_old_set - sorted_files_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28e0594f-5cd0-48a0-ac9c-fb4490005029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 13721 files found exclusively in data_old\n",
      "We have sorted 12877 of these files into folders for data\n",
      "This leaves 844 unsorted files\n",
      "Does this add up? 12799 + 922 = 13721\n"
     ]
    }
   ],
   "source": [
    "print('There were ' + str(len(only_in_old_set)) + ' files found exclusively in data_old')\n",
    "print('We have sorted ' + str(len(sorted_files_set)) + ' of these files into folders for data')\n",
    "print('This leaves ' + str(len(unsorted_files)) + ' unsorted files')\n",
    "print('Does this add up? 12799 + 922 = ' + str(12799 + 922))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b9eff-f264-4a9a-bfb3-f4adb61b4019",
   "metadata": {},
   "source": [
    "Saving our dicts to txt files so that we can use these lists to extract files from data_old to specified directories in data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82571289-bdb9-4049-95c8-0c22dde77370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "620"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1600 - 105 - 475 - 200 -200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d218be-07db-40db-812e-e8c5cac039e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(fiwi)",
   "language": "python",
   "name": "fiwi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
